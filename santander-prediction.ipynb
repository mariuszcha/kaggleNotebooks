{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mariuszcha/santander-prediction-v1?scriptVersionId=128894821\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/10385/logos/thumb76_76.png?t=2019-02-15-16-53-52)The aim of this project is to predict which customers of Santander Bank will carry out a transaction, using a dataset provided by the institution. The data of each customer has been anonymized to protect their privacy. The data provided for this competition has the same structure as the real data we(Santander's Scientists) have available to solve this problem. To assess the quality of our predictions, we will be using the ROC AUC (Receiver Operating Characteristic Area Under Curve) metric, which allows for evaluating the performance of a model by analyzing the ROC curve and calculating the area under the curve.","metadata":{}},{"cell_type":"markdown","source":"![](https://images.pexels.com/photos/351264/pexels-photo-351264.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2)","metadata":{}},{"cell_type":"markdown","source":"Our evaluation matric is **ROC AUC Curve.**\nThe ROC AUC Curve is a way to evaluate the performance of a binary classification model, and provides a visual representation of the trade-off between the True Positive Rate and False Positive Rate. A high AUC score and a curve that is as close to the top left corner of the graph as possible indicate good performance of the model.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom lightgbm import LGBMClassifier\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nsns.set_palette('flare')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-09T12:01:13.5572Z","iopub.execute_input":"2023-05-09T12:01:13.560193Z","iopub.status.idle":"2023-05-09T12:01:13.577983Z","shell.execute_reply.started":"2023-05-09T12:01:13.560128Z","shell.execute_reply":"2023-05-09T12:01:13.576918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading test data and train data.\ntrain_df = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/train.csv', index_col=0)\ntest_df = pd.read_csv('/kaggle/input/santander-customer-transaction-prediction/test.csv', index_col=0)\n\n# Creating list of column names.\nfeatures = [x for x in train_df if x.startswith('var')]\n\n#X and y\nX = train_df[features]\ny = train_df.target","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:01:13.580543Z","iopub.execute_input":"2023-05-09T12:01:13.581107Z","iopub.status.idle":"2023-05-09T12:01:25.785238Z","shell.execute_reply.started":"2023-05-09T12:01:13.581075Z","shell.execute_reply":"2023-05-09T12:01:25.78428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n**In our first step we will examine whether there are Missing Values in our data sets**","metadata":{}},{"cell_type":"code","source":"def isMissing(df):\n    missing_values = df.isna().sum()\n    total_missings = missing_values.sum()\n    print('Number of NAs in set:', total_missings)\n\nisMissing(train_df)\nisMissing(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:01:25.78651Z","iopub.execute_input":"2023-05-09T12:01:25.787063Z","iopub.status.idle":"2023-05-09T12:01:25.917556Z","shell.execute_reply.started":"2023-05-09T12:01:25.787034Z","shell.execute_reply":"2023-05-09T12:01:25.916458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above - we counted missing values across whole set - and we can notice that there aren't any missing values\n\n**Now we will check distributions of data for independent variables and for dependent one. We could plot all of them but that requires a lot of CPU Usage. As we are builing the model in order to learn something new - we will select TOP15 important features according to Correlation Matrix and work mostly based on them. We will examine te STD, Mean, MAX, MIN using .describe()**","metadata":{}},{"cell_type":"code","source":"# We create list of TOP 15 features\nsignificant_features = train_df.corr().abs()['target'].sort_values(ascending=False)[1:16].index.tolist()\ndisplay(train_df.describe())\ndisplay(test_df.describe())","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:01:25.919039Z","iopub.execute_input":"2023-05-09T12:01:25.919364Z","iopub.status.idle":"2023-05-09T12:01:52.403675Z","shell.execute_reply.started":"2023-05-09T12:01:25.919335Z","shell.execute_reply":"2023-05-09T12:01:52.402523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from tables above for Test data and Train data and from graphs below - our data is mostly normal distributed. There isn't any skewness. We can have some outliers values so to scale data - we will use RobustScaler. Obviously we don't know the whole set. Maybe the rest variables are less normal - but the ones which are crucial for modeling are fine.\nOne of concerns i have is Kurtosis. We can notice that some of them have problem with that indicator.\nKurtosis helps to identify the presence of outliers or extreme values in a dataset.\nA distribution with positive kurtosis has heavier tails than a normal distribution, which means it has more outliers or extreme values. A distribution with negative kurtosis has lighter tails than a normal distribution, which means it has fewer outliers or extreme values.\n","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3,5,figsize=(15,10))\naxes = axes.flatten()\n\nfor f,i in enumerate(significant_features):\n    sns.histplot(kde=True, x=train_df[i], ax=axes[f])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:01:52.407203Z","iopub.execute_input":"2023-05-09T12:01:52.407875Z","iopub.status.idle":"2023-05-09T12:02:13.327241Z","shell.execute_reply.started":"2023-05-09T12:01:52.407831Z","shell.execute_reply":"2023-05-09T12:02:13.325986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will check distribution of target variable","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=train_df['target'])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:13.328833Z","iopub.execute_input":"2023-05-09T12:02:13.32927Z","iopub.status.idle":"2023-05-09T12:02:13.578911Z","shell.execute_reply.started":"2023-05-09T12:02:13.329229Z","shell.execute_reply":"2023-05-09T12:02:13.577523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is kinda big problem. We can notice that our classes are unbalanced. Solutions for problem like that is oversampling, undersampling or randomsampling.","metadata":{}},{"cell_type":"markdown","source":"**We will check now distribution Train data vs Test data for all rows.\nOne method of comparing the distributions between the train and test sets is to calculate the mean, median, standard deviation, etc. for each column in the train set and compare the same values for the test set. If the means or standard deviations differ significantly between the train and test sets, it may indicate problems with the model's generalization.**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2,2, figsize=(15,7))\naxes=axes.flatten()\n\nsns.histplot(x=train_df[features].mean(), kde=True, ax=axes[0], bins=50, color='b')\nsns.histplot(x=test_df[features].mean(), kde=True, ax=axes[0], bins=50)\n\nsns.histplot(x=train_df[features].median(), kde=True, ax=axes[1], bins=50, color='b')\nsns.histplot(x=test_df[features].median(), kde=True, ax=axes[1], bins=50)\n\nsns.histplot(x=train_df[features].std(), kde=True, ax=axes[2], bins=50, color='b')\nsns.histplot(x=test_df[features].std(), kde=True, ax=axes[2], bins=50)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:13.58269Z","iopub.execute_input":"2023-05-09T12:02:13.583144Z","iopub.status.idle":"2023-05-09T12:02:17.991638Z","shell.execute_reply.started":"2023-05-09T12:02:13.583112Z","shell.execute_reply":"2023-05-09T12:02:17.990623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see - from graphs - we shouldn't have problem with generalization in data. Both - test and train sets - are quite similar.","metadata":{}},{"cell_type":"markdown","source":"**We will also check if there are columns wich values duplicate. That would mean that the variance of given column is low and. Maybe we could find a similar pattern in test columns and just use it to assign proper class?**","metadata":{}},{"cell_type":"code","source":"display(train_df.var().sort_values(ascending=True)[0:10])\ndisplay(test_df.var().sort_values(ascending=True)[0:10])","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:17.992874Z","iopub.execute_input":"2023-05-09T12:02:17.99318Z","iopub.status.idle":"2023-05-09T12:02:18.862696Z","shell.execute_reply.started":"2023-05-09T12:02:17.993153Z","shell.execute_reply":"2023-05-09T12:02:18.861622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see - there are variables with very low variance both on train and test data. That could be important thing for the future.","metadata":{}},{"cell_type":"markdown","source":"**To sum up:\na) We have to use RobustScaler on our data to see if it will improve our scores\nb) We will use LightGBM for now as an our model**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_df[features], train_df['target'], random_state=13, test_size=0.2)\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:18.86439Z","iopub.execute_input":"2023-05-09T12:02:18.864831Z","iopub.status.idle":"2023-05-09T12:02:20.758309Z","shell.execute_reply.started":"2023-05-09T12:02:18.864791Z","shell.execute_reply":"2023-05-09T12:02:20.757354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LGBMClassifier(random_state=13, n_jobs=-1, class_weight='balanced')\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\nfpr, tpr, threshold = roc_curve(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:20.759611Z","iopub.execute_input":"2023-05-09T12:02:20.760763Z","iopub.status.idle":"2023-05-09T12:02:40.419053Z","shell.execute_reply.started":"2023-05-09T12:02:20.760724Z","shell.execute_reply":"2023-05-09T12:02:40.417899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(fpr, tpr, color='green', label='ROC Curve')\nplt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Baseline')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\ndisplay('roc_auc_score:', roc_auc_score(model.predict(X_test), y_test))\ndisplay('accuracy:', model.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:40.420432Z","iopub.execute_input":"2023-05-09T12:02:40.420864Z","iopub.status.idle":"2023-05-09T12:02:41.091256Z","shell.execute_reply.started":"2023-05-09T12:02:40.420828Z","shell.execute_reply":"2023-05-09T12:02:41.090201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see - for LightGBM ROC AUC Score is 65%. But the most important thing is how it is going to perform on main validation data where we don't know target values.","metadata":{}},{"cell_type":"code","source":"# That part is about evaluating our model on new test data and check score thru Kaggle Competitions\nx = scaler.transform(test_df)\npredictions = model.predict(x)\nsubmission = pd.DataFrame({'ID_code': test_df.index, 'target': predictions})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-09T12:02:41.094977Z","iopub.execute_input":"2023-05-09T12:02:41.095368Z","iopub.status.idle":"2023-05-09T12:02:42.5474Z","shell.execute_reply.started":"2023-05-09T12:02:41.095334Z","shell.execute_reply":"2023-05-09T12:02:42.546368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy for test set is 79%. It's not terrible but we will definitely try to improve our score in future. Top scientists on leaderboard do about 90%.","metadata":{}},{"cell_type":"markdown","source":"# To do...\n* Explore more relations between variables - test variables too\n* Improve competition score","metadata":{}}]}